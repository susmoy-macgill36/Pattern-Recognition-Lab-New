# -*- coding: utf-8 -*-
"""Perceptron-PatternLab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y16wZ2mQxrhF3lo8cfAd-BwQcxPZ3lql

**Assignment 2**

**Name:“Implementing the Perceptron algorithm for finding the weights of a Linear Discriminant function.”**

**Submitted By:**

**Name: Susmoy Chakraborty**

**ID: 15-02-04-114**

**Section: B**

**Group: B2**

**Submission Date: 30-8-2019**

**IDE: Colab notebook**
"""

#importing all header files

import pandas as pd
import numpy as np

from matplotlib import pyplot as plt
np.random.seed(123)

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

"""**1.Take input from “train.txt” file. Plot all sample points from both classes, but samples from the same class should have the same color and marker. Observe if these two classes can be separated with a linear boundary.**"""

df = pd.read_csv('train.txt', delimiter =" ",names =['x','y', 'classname'])

x_class1=[]
x_class2 =[]
y_class1 =[]
y_class2 =[]

for i in range(len(df)):
    if df['classname'][i]==1:
        x_class1.append(df['x'][i])
        y_class1.append(df['y'][i])
    else:
        x_class2.append(df['x'][i])
        y_class2.append(df['y'][i])

plt.scatter(x_class1,y_class1, color='r',marker='o',label='class 1')
plt.scatter(x_class2,y_class2, color='g',marker='+',label='class 2')

     
        
plt.legend()
plt.show()

class1_mean=np.array([(sum(x_class1)/len(x_class1)) , (sum(y_class1)/len(y_class1))])
class2_mean=np.array([(sum(x_class2)/len(x_class2)) , (sum(y_class2)/len(y_class2))])

mean_difference = class1_mean.T-class2_mean.T

c=-0.5*(class1_mean.dot(class1_mean.T)- class2_mean.dot(class2_mean.T))

allclass_xy =[]

for i in range(len(df)):
    allclass_xy.append(df['x'][i])
    allclass_xy.append(df['y'][i])

allclass_xy

x1 = np.linspace(min(allclass_xy),max(allclass_xy),50)

x2 =[]


for i in range(len(x1)):
      res = - (((mean_difference[0]*x1[i])+c)/mean_difference[1])
      x2.append(res)

plt.scatter(x_class1,y_class1, color='r',marker='o',label='class 1')
plt.scatter(x_class2,y_class2, color='g',marker='+',label='class 2')
plt.scatter(x1,x2, color='b',marker='*',label='Decision Boundary') #db
plt.legend()
plt.show()

"""**2. Consider the case of a second order polynomial discriminant function. Generate the high dimensional sample points y, as discussed in the class.**"""

class1 =[]
class2 =[]
for i in range(len(x_class1)):
    class1.append(x_class1[i])
    class1.append(y_class1[i])

for i1 in range(len(x_class2)):
    class2.append(x_class2[i1])
    class2.append(y_class2[i1])

class1_shaped =np.array(class1).reshape(len(class1)//2,2)
class2_shaped =np.array(class2).reshape(len(class2)//2,2)

y1=[]
y2 =[]
class1_highDim=[]
class2_highDim =[]
print("class 1 high dimension: \n")
for i in range(len(class1_shaped)):
    x1=class1_shaped[i,:][0]
    x2 =class1_shaped[i,:][1]
    y1 = [x1*x1,x2*x2,x1*x2,x1,x2,1.0]
    print(y1)
    class1_highDim.append(y1)

print("class 2 high dimension: \n")
for i in range(len(class2_shaped)):
    x1=class2_shaped[i,:][0]
    x2 =class2_shaped[i,:][1]
    y2 = [x1*x1,x2*x2,x1*x2,x1,x2,1.0]
    print(y2)
    class2_highDim.append(y2)

print("----Normalization of class2---- \n")

class2_highDim_norm =[]

for i in range(len(class2_highDim)):
    #print(np.array(class2_highDim[i]) *(-1.0))
    class2_highDim_norm.append(np.array(class2_highDim[i]) *(-1.0))

class2_highDim_norm

"""**3. Use Perceptron Algorithm (both one at a time and many at a time) for finding the weight-coefficients of the discriminant function (i.e., values of w) boundary for your linear classifier in task 2. Here α is the learning rate and 0 < α ≤ 1.**

**4. Three initial weights have to be used (all one, all zero, randomly initialized with seed fixed). For all of these three cases vary the learning rate between 0.1 and 1 with step size0.1. Create a table which should contain your learning rate, number of iterations for one at a time and batch Perceptron for all of the three initial weights. You also have to create a bar chart visualizing your table data.**
"""

alpha = np.linspace(0.1,1,10)

all_classes_hd = []

for i in range(len(class1_highDim)):
    all_classes_hd.append(np.array(class1_highDim[i]))
    
for j in range(len(class2_highDim_norm)):
        
    all_classes_hd.append(class2_highDim_norm[j])

all_classes_hd

all_wT = np.array([np.ones(6),np.zeros(6),np.random.rand(6)])

all_wT

"""**Many at a time**"""

many_at_time =[]

for a2 in range(len(all_wT)):  
    for a in range(len(alpha)):
       
        wT=all_wT[a2]
        number_of_itr=0
           
        for i in range(300):
                number_of_itr = number_of_itr+1 
                neg =[]

                for i1 in range(len(all_classes_hd)):
                    jk=np.array(wT).dot(all_classes_hd[i1])
                    if (jk <=0):
                        #print(jk)
                        neg.append(all_classes_hd[i1])
                if (len(neg)==0):
                     break
            
                wTT = wT+ (alpha[a] * (np.sum(neg,axis=0)))
           
                #print(wTT)
                wT= wTT 
                #number_of_itr = number_of_itr+1    
        print(number_of_itr)
        print(wTT)
        many_at_time.append(number_of_itr)
    print('_________________________________________________')

"""**One at a time**"""

one_at_a_time =[]

for a2 in range(len(all_wT)):  
    for a in range(len(alpha)):
        wT=all_wT[a2]
        number_of_itr=0
        for i in range(200):
               number_of_itr=number_of_itr+1
               neg=[]
               for i1 in range(len(all_classes_hd)):
                        #print(wT) 
                        jk=np.array(wT).dot(all_classes_hd[i1])
           
                        if (jk<=0):
                            wTT1 = np.add(wT,alpha[a] * all_classes_hd[i1])
                            neg.append(all_classes_hd[i1])
                        else:
                            wTT1 = wT
                        wT= wTT1
             
              
    
               if len(neg)==0:
                       break
        print(number_of_itr)
        print(wT)
        one_at_a_time.append(number_of_itr)
    print('_________________________________________________')

many_at_time1 = np.array(many_at_time).reshape(len(many_at_time)//len(alpha),len(alpha))

one_at_a_time1 = np.array(one_at_a_time).reshape(len(one_at_a_time)//len(alpha),len(alpha))

def plotting(index1):
    width =0.03
    plt.axis([0.0, 1.1, 0, max(max(many_at_time),max(one_at_a_time))+10])
    plt.bar(alpha,one_at_a_time1[index1],width=width,color='r',label='One at a time')
    plt.bar(alpha+width,many_at_time1[index1],width=width,color='g',label='Many at a time')
    plt.xlabel("learning rate")
    plt.ylabel("Number of iterations")
    plt.legend()

"""**Case 1: Initial Weight Vector All One**"""

allone = pd.DataFrame({
    'Value of alpha (Learning Rate)': alpha,
     'One at a time ':one_at_a_time1[0] ,
     'Many at a time': many_at_time1[0]
    })

allone

plotting(0)

"""**Case 2: Initial Weight Vector All Zero**"""

allzero = pd.DataFrame({
    'Value of alpha (Learning Rate)': alpha,
     'One at a time ':one_at_a_time1[1] ,
     'Many at a time': many_at_time1[1]
    })

allzero

plotting(1)

"""**Case 3: Initial Weight Vector All Random**"""

allrandom = pd.DataFrame({
    'Value of alpha (Learning Rate)': alpha,
     'One at a time ':one_at_a_time1[2] ,
     'Many at a time': many_at_time1[2]
    })

allrandom

plotting(2)

